{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 6\n",
    "## Quiz\n",
    "\n",
    "### **Question 1: K-Nearest Neighbors (KNN) Classification**\n",
    "In K-Nearest Neighbors (KNN), what is the primary role of the hyperparameter $k$?\n",
    "\n",
    "A. It determines the maximum distance between neighbors to consider in the classification.  \n",
    "B. It specifies the number of nearest data points to use when assigning a class.  \n",
    "C. It controls the regularisation of the decision boundary.  \n",
    "D. It adjusts the weights of features during distance calculation.  \n",
    "\n",
    "**Correct Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### **Question 2: Bayes’ Theorem**\n",
    "Which of the following best describes the interpretation of Bayes’ Theorem?\n",
    "\n",
    "A. It calculates the likelihood of an event occurring based solely on prior knowledge.  \n",
    "B. It updates the probability of a hypothesis based on new evidence.  \n",
    "C. It assumes all events are independent and calculates probabilities accordingly.  \n",
    "D. It estimates the most likely outcome by averaging all observed outcomes.  \n",
    "\n",
    "**Correct Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### **Question 3: Naive Bayes Classification**\n",
    "What does the \"naive\" assumption in Naive Bayes classifiers refer to?\n",
    "\n",
    "A. Features are assumed to be independent of each other given the class label.  \n",
    "B. The model assumes equal probabilities for all classes.  \n",
    "C. The algorithm uses simple rules without considering probabilities.  \n",
    "D. All data points are assumed to have equal weights.  \n",
    "\n",
    "**Correct Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### **Question 4: Support Vector Machines (SVM)**\n",
    "In the context of Support Vector Machines (SVM), which of the following statements about hyperplanes and linear separability is true?\n",
    "\n",
    "A. A hyperplane is a line that separates data points in 2D space but cannot generalise to higher dimensions.        \n",
    "B. If data is not linearly separable in its original space, SVM can map it to a higher-dimensional space where it may become separable.     \n",
    "C. Linear separability implies that no support vectors are needed to define the hyperplane.     \n",
    "D. Hyperplanes can only exist in datasets with an equal number of features and samples.\n",
    "\n",
    "**Correct Answer:** \n",
    "\n",
    "---\n",
    "\n",
    "### **Question 5: Soft Margins**\n",
    "In the context of Support Vector Machines (SVM), what does a soft margin allow?\n",
    "\n",
    "A. Misclassifications of some training points to achieve a better trade-off between margin size and classification accuracy.    \n",
    "B. The use of kernels to transform data into higher dimensions.     \n",
    "C. The use of probabilities instead of deterministic class boundaries.  \n",
    "D. All training data points to fall exactly on the decision boundary. \n",
    "\n",
    "\n",
    "**Correct Answer:** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will create an SVM to classify synthetic data. Students can explore a multi-class dataset with both linear and non-linear features.\n",
    "\n",
    "### **Exercise Overview:**\n",
    "- **Objective:** Train an SVM model to classify data points into three classes.\n",
    "- **Dataset Description:**\n",
    "   - `svm_dataset.csv`: Use this dataset to train and test your models.\n",
    "   - `svm_valid_dataset.csv`: Use this dataset in the last exercise. Don't use this dataset to tune your model.\n",
    "- **Data Description:**\n",
    "   - Features:\n",
    "     - `Feature_1` to `Feature_4`: The original normally distributed input features.\n",
    "   - Label: `Label` (class identifier: 0, 1, or 2).\n",
    "- **Tasks:**\n",
    "   1. **Train-Test Split** Load `svm_dataset.csv` and standardise the features using [StandardScaler](https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.StandardScaler.html). \n",
    "   2. **Train:** Train an SVM model with different kernels (`linear` vs `rbf`) and compare performances of the kernels in terms of accuracy.\n",
    "   3. **Cross Validation:** Perform k-fold cross-validation (e.g., $k=5$) on the training dataset to evaluate the stability and generalisation of your SVM model. Calculate and report the average accuracy and standard deviation across folds.\n",
    "   4. **Fine tuning:** Optimise hyperparameters such as `C` and `gamma` for the `rbf` kernel.\n",
    "   5. **Validate:** Choose your final model (justify your choice) and test it on the validation set `svm_valid_dataset.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
